---
title: "DTSC 5301 PROJECT"
author: ''
date: "9/2/2021"
output:
  pdf_document: default
  html_document: default
---

# Question of Interest
### We want to know whether or not the stimulus checks sent out by the US government have had a positive impact on the economy (using consumer spending as a proxy for how healthy the economy is).

# Data Source
### All of our data was aggregated by Opportunity Insights at https://github.com/OpportunityInsights/EconomicTracker.  In this analysis, we use spending data provided by Affinity Solutions, job postings data from Burning Glass Technologies, COVID data from the CDC, GPS mobility reports from Google, unemployment claims from the Department of Labor, and employment levels from Paychex, Intuit, Earnin and Kronos.

**Primary Reference:**

"The Economic Impacts of COVID-19: Evidence from a New Public Database Built Using Private Sector Data", by Raj Chetty, John Friedman, Nathaniel Hendren, Michael Stepner, and the Opportunity Insights Team. November 2020. Available at: https://opportunityinsights.org/wp-content/uploads/2020/05/tracker_paper.pdf

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
```

## Read in Data from GitHub Repository
```{r get_data, cache = TRUE, message = FALSE}
covid_daily_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/COVID%20-%20State%20-%20Daily.csv")

move_daily_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Google%20Mobility%20-%20State%20-%20Daily.csv")

affinity_daily_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Affinity%20-%20State%20-%20Daily.csv")

job_listings_weekly_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Burning%20Glass%20-%20State%20-%20Weekly.csv")

employment_daily_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Employment%20-%20State%20-%20Daily.csv")

ui_claims_weekly_df <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/UI%20Claims%20-%20State%20-%20Weekly.csv")

state_id <- read_csv("https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/GeoIDs%20-%20State.csv")

#stimulus <- read_csv("")
```
## Join the datasets we're interested in into one dataset.

Here we join the datasets of interest based on a shared date and state of measurements.  We are joining weekly datasets (job listings and unemployment insurance claims) with the daily datasets.  This will leave a bunch of **NA** values.  We'll come back to fix that later.

```{r join_tibbles}
df <- left_join(affinity_daily_df, move_daily_df, by = c("year", "month", "day", "statefips"))

df <- left_join(df, covid_daily_df, by = c("year", "month", "day", "statefips"))

df <- left_join(df, employment_daily_df, by = c("year", "month", "day", "statefips"))

df <- full_join(df, job_listings_weekly_df, by = c("year", "month", "day" = "day_endofweek", "statefips"))

df <- full_join(df, ui_claims_weekly_df, by = c("year", "month", "day" = "day_endofweek", "statefips"))

df <- left_join(df, state_id, by = c("statefips"))
```

### Combine "month", "day", and "year" columns into a "date" column
```{r clean_dates}
# https://tidyr.tidyverse.org/reference/unite.html
df <- df %>% unite("date", day:month:year, remove = FALSE, sep = "-")

# https://lubridate.tidyverse.org/reference/ymd.html
df$date <- dmy(df$date)

df <- df %>% mutate(week = week(date))
```

### Data Selection and Cleaning

In this code chunk, we select the columns that we're interested in from the dataframe that we joined a couple of steps ago.  We could use all of the data in the dataframe, but we choose not to, since not all of the features will be helpful in answering the question of whether or not the stimulus checks have boosted the economy.

```{r condense_data, warning = FALSE}

df <- df %>%
  select(date, year, month, day, week, statename, stateabbrev, state_pop2019, initclaims_rate_regular, contclaims_rate_combined, bg_posts, emp, spend_all, gps_retail_and_recreation, gps_grocery_and_pharmacy, gps_parks, gps_transit_stations, gps_workplaces, gps_residential, gps_away_from_home, new_case_count, new_death_count, case_count, death_count) %>%
  mutate(
    spend_all = as.double(spend_all),
    gps_parks = as.double(gps_parks),
    new_case_count = as.double(new_case_count),
    new_death_count = as.double(new_death_count),
    case_count = as.double(case_count),
    death_count = as.double(death_count),
    gps_transit_stations = as.double(gps_transit_stations),
    emp = as.double(emp),
    contclaims_rate_combined = as.double(contclaims_rate_combined)
  )


glimpse(df)
```

To fix the **NA** values found in our dataset, we replace them all with **0**.  We could have handled them many other ways, like replacing them with the column mean, median, mode, or by training a regression model to fill them based on the rows that were not missing those values.  For this dataset, though, we found that **NA**s are frequently used when the there was no interesting data to report (in other words, the value for the feature was zero).  This can be seen particularly in columns like new_death_count and new_case_count from the CDC COVID dataset.

We also drop rows that have 0 spending data because this value is not realistic.  Also, we need the spend_all column to be clean because we will be using it for plotting and training a regression model later on.

```{r remove_na}
# https://stackoverflow.com/questions/45576805/how-to-replace-all-na-in-a-dataframe-using-tidyrreplace-na

#length(df$date)

#colSums(is.na(df))

df <- df %>% replace(is.na(.), 0)
df <- df %>% filter(spend_all != 0)


```

### Combing Weekly and Daily Data

In this code chunk, we aggregate daily data into weekly data.  For most columns, the appropriate aggregate function is **mean()**, with the exception of the *new_case_count*, *new_death_count*, *case_count*, *death_count*, and *date columns*.

We also add in additional data about the states, at this point (like state name, abbreviation, and population).

```{r}
df_weekly <- df %>%
  group_by(year, week, stateabbrev) %>%
  summarize(spend_all = mean(spend_all), contclaims_rate_combined = mean(contclaims_rate_combined), bg_posts = mean(bg_posts), emp = mean(emp), gps_retail_and_recreation = mean(gps_retail_and_recreation), gps_grocery_and_pharmacy = mean(gps_grocery_and_pharmacy), gps_parks = mean(gps_parks), gps_transit_stations = mean(gps_transit_stations), gps_workplaces = mean(gps_workplaces), gps_residential = mean(gps_residential), gps_away_from_home = mean(gps_away_from_home), new_case_count = sum(new_case_count), new_death_count = sum(new_death_count), case_count = max(case_count), death_count = max(death_count), date = max(date))

df_weekly <- left_join(df_weekly, state_id, by = c("stateabbrev"))
```
```{r}

```

### Adding Stimulus Check Data

Here we add in the data for the COVID stimulus checks.  We do this by creating a feature encoding for each check, where the value for that check is **0** before the check is sent out, and **1** after the check is sent out.  This process created three new features, which we creatively named *first_check*, *second_check*, and *third_check*

```{r add_stimulus_checks, warning = FALSE}

df_weekly <- df_weekly %>% mutate(first_check = (if (date < ymd("2020-04-15")) 0 
                                                 else 1), 
                                  second_check = (if (date < ymd("2021-01-04")) 0 
                                                  else 1), 
                                  third_check = (if (date < ymd("2021-03-18")) 0 
                                                 else 1))

```

### Double Check that Data is Clean

We already took care of **NA** values a few steps ago.  Now we need to ensure that there are no infinite values, as well.

```{r}
sum(sapply(df_weekly, is.infinite))
```
As you can see from the results above, the number of infinite values in our dataset currently is `r sum(sapply(df_weekly, is.infinite))`.

### Training a Linear Model on Our Data

In this code chunk, we fit a linear model to the *gps_retail_and_recreation*, *emp*, *first_check*, *second_check*, and *third_check* features, with the goal of predicting the *spend_all* variable.  The working assumption here is that *spend_all* is a dependent variable, and the others are independent variables.

```{r}
lm <- lm(spend_all ~ gps_retail_and_recreation + emp + first_check + 
           second_check + third_check, df_weekly)
```

```{r}
summary(lm)
```
### Summarize the Model

Based on the information displayed above, all of the variables we are regressing on are statistically significant for predicting the overall spending.


### Plotting spending over time for all states and categories

The dates for the stimulus checks were approximated from [this article](https://en.as.com/en/2021/08/25/latest_news/1629920433_478504.html).  

```{r plot_spend_smooth, cache = TRUE}

# https://stackoverflow.com/questions/38815996/r-adding-geom-vline-labels-to-geom-histogram-labels

ggplot(df_weekly, aes(x = date, y = spend_all)) +
  geom_smooth() +
  geom_vline(xintercept = as.Date("2020-04-12")) +
  geom_vline(xintercept = as.Date("2021-01-01")) +
  geom_vline(xintercept = as.Date("2021-03-01")) +
  geom_text(aes(x = as.Date("2020-05-28"), label = "1st Check"), color = "red", angle = 45, y = 0) +
  geom_text(aes(x = as.Date("2020-11-05"), label = "2nd Check"), color = "dark green", angle = 45, y = -.1) +
  geom_text(aes(x = as.Date("2021-04-25"), label = "3rd Check"), color = "blue", angle = 45, y = -.1) +
  labs(title = "What We Wished Our Data Looked Like", subtitle = "Spending Over Time (Compared to Pre-Pandemic Baseline)")
```

```{r plot_spend_all, cache = TRUE}

# https://stackoverflow.com/questions/38815996/r-adding-geom-vline-labels-to-geom-histogram-labels

ggplot(df, aes(x = date, y = spend_all)) +
  geom_point() +
  geom_smooth() +
  geom_vline(xintercept = as.Date("2020-04-12")) +
  geom_vline(xintercept = as.Date("2021-01-01")) +
  geom_vline(xintercept = as.Date("2021-03-01")) +
  geom_text(aes(x = as.Date("2020-05-28"), label = "1st Check"), color = "red", angle = 45, y = 0) +
  geom_text(aes(x = as.Date("2020-11-05"), label = "2nd Check"), color = "dark green", angle = 45, y = -.1) +
  geom_text(aes(x = as.Date("2021-04-25"), label = "3rd Check"), color = "blue", angle = 45, y = -.1) +
  labs(title = "What Our Data Looks Like", subtitle = "Spending Over Time (Compared to Pre-Pandemic Baseline)")
```

```{r}
ggplot(df, aes(x = gps_retail_and_recreation, y = spend_all, color = stateabbrev)) +
  geom_point()
```






